{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db43782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fc0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bearer_token(file_path = 'bearer_token.txt'):\n",
    "    \"\"\"\n",
    "    This is a helper function that reads a bearer token form\n",
    "    a specified file path into a string object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Readable file path containing a document which includes\n",
    "        the Twitter API bearer token in its first line. Defaults\n",
    "        to 'bearer_token.txt'.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The Twitter API bearer token as a string object.\n",
    "    \"\"\"\n",
    "    # Will raise error if path does not exist\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            BEARER_TOKEN = line.strip()\n",
    "            break\n",
    "    f.close() \n",
    "    return BEARER_TOKEN\n",
    "\n",
    "def look_up_twitter_acount_id(BEARER_TOKEN, user_name):\n",
    "    \"\"\"\n",
    "    This is a helper function to set a simple Twitter API \n",
    "    request to look up a Twitter user ID based on a user\n",
    "    handle (e.g., '@twitter' or 'nhsuk'). The user can both\n",
    "    parse in the username as a handle (string including '@') \n",
    "    and as a string without the '@'. The Twitter user ID\n",
    "    is needed for subsequent Twitter API calls.\n",
    "    \n",
    "    GET /2/users/by/username/:username\n",
    "    \n",
    "    App rate limit: 300 requests per 15-minute window\n",
    "    \n",
    "    https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users-by-username-username\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    BEARER_TOKEN : str\n",
    "        A Twitter API bearer token as a string object.\n",
    "    user_name : str\n",
    "        The username to look up (either including or \n",
    "        excluding) the '@' symbol.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The Twitter user ID as a string object.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ApiError\n",
    "        Either the request status code was not 200\n",
    "        or the requested user name was malformed.\n",
    "    \"\"\"\n",
    "    # If a handle was parsed, convert to user name\n",
    "    if (user_name[0] == \"@\"):\n",
    "        user_name = user_name[1:]\n",
    "    \n",
    "    # If user name does not only contain letters, numbers, or\n",
    "    # underscores, raise error\n",
    "    if not re.match(\"^[\\w\\d_]*$\", user_name):\n",
    "        raise ApiError('The user name you requested seems to be malformed.')\n",
    "    \n",
    "    s = requests.Session()\n",
    "    s.headers.update({'Authorization': f'Bearer {BEARER_TOKEN}'})\n",
    "\n",
    "    req = s.get(f'https://api.twitter.com/2/users/by?usernames={user_name}')\n",
    "    time.sleep(3) # rate limit\n",
    "    \n",
    "    if req.status_code != 200:\n",
    "        raise ApiError(f'There was an error sending the request. '\\\n",
    "                       f'Status code {req.status_code}')\n",
    "    \n",
    "    page = json.loads(req.content)\n",
    "\n",
    "    twitter_id = page['data'][0]['id']\n",
    "    \n",
    "    return twitter_id\n",
    "\n",
    "def get_most_recent_tweets_account(ACCOUNT_ID, BEARER_TOKEN, PARAMS, \n",
    "                                   verbose=True, save_file=True,\n",
    "                                   file_reference='DOWNLOAD'):\n",
    "    \"\"\"\n",
    "    This function is a download routine to get the most recent \n",
    "    Tweets of a specified Twitter account. It extracts the pagination \n",
    "    from the results of the API calls until either less than 100 \n",
    "    results are returned or 32 API calls have been made (which is \n",
    "    the maximum number of tweets Twitter allows academic researchers \n",
    "    to download as of November 2021). The function uses the \n",
    "    GET /2/users/:id/tweets endpoint of the Twitter V2 API. The \n",
    "    endpoint allows for 900 requests per 15-minute window. Hence, \n",
    "    the thread pauses for 1 second in between API calls. More \n",
    "    information on the endpoint can be found here:\n",
    "    https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/api-reference/get-users-id-tweets\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ACCOUNT_ID : str\n",
    "        A Twitter user ID.\n",
    "    BEARER_TOKEN : str\n",
    "        A Twitter API bearer token.\n",
    "    PARAMS : dict\n",
    "        A dictionary parsed to the header of the API URL request\n",
    "    verbose : bool\n",
    "        A boolean indicating whether progress of the download\n",
    "        routine should be printed to the console. Defaults to\n",
    "        True.\n",
    "    save_file : bool\n",
    "        A boolean indicating whether the resulting data frame\n",
    "        with all tweets should be saved to a csv file including\n",
    "        a timestamp in the file name (since download output\n",
    "        may depend on the time of download). Defaults to True.\n",
    "    file_reference : str\n",
    "        Reference string that should appear in the output\n",
    "        file name if the results should be saved.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pandas data frame including all tweets with one row\n",
    "        representing one tweet and the variables specified in\n",
    "        the PARAMS argument parsed through the\n",
    "        pandas.json_normalize() function.\n",
    "        \n",
    "    Raises\n",
    "    ------\n",
    "    ApiError\n",
    "        Either the request status code was not 200\n",
    "        or the PARAMS argument is malformed (but not\n",
    "        invalid) to ensure that the pagination routine\n",
    "        works as intended.\n",
    "    \"\"\"\n",
    "    if 'pagination_token' in PARAMS.keys():\n",
    "        raise ApiError('The parsed query parameters included a pagination '\\\n",
    "                       'token. Check your PARAMS argument.')\n",
    "        \n",
    "    if 'max_results' not in PARAMS.keys() or int(PARAMS['max_results']) != 100:\n",
    "        raise ApiError('Please ensure that you parse max_results: 100 to '\\\n",
    "                       'your requests parameters.')\n",
    "    \n",
    "    # Prepare URL request\n",
    "    s = requests.Session()\n",
    "    s.headers.update({'Authorization': f'Bearer {BEARER_TOKEN}'})\n",
    "    URL = f\"https://api.twitter.com/2/users/{ACCOUNT_ID}/tweets\"\n",
    "    request_count = 0\n",
    "    \n",
    "    while (request_count < 32):\n",
    "        req = requests.models.PreparedRequest()\n",
    "        req.prepare_url(URL, PARAMS)\n",
    "        req = s.get(req.url)\n",
    "        # Sleep for 1 second due to rate limit\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if req.status_code != 200:\n",
    "            raise ApiError(f'There was an error sending request '\\\n",
    "                           f'{request_count+1}. Status code '\\\n",
    "                           f'{req.status_code}')\n",
    "\n",
    "        # Get content, paginate and save rows to df\n",
    "        page = json.loads(req.content)\n",
    "        \n",
    "        if page['meta']['result_count'] == 0 and request_count == 0:\n",
    "            if verbose:\n",
    "                print(f'No results found for {file_reference}! Returning '\\\n",
    "                      f'empty data frame.')\n",
    "            if save_file:\n",
    "                print('Note: Results will not be written to a '\\\n",
    "                      'timestamped file but an empty file will '\\\n",
    "                      'be created for future reference.')\n",
    "                open(f'data/{file_reference}_empty.csv', 'a').close()\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # If it is the first request, initialize data frame\n",
    "        if request_count == 0: \n",
    "            df = pd.json_normalize(page['data'])\n",
    "        # Else append results to existing data frame\n",
    "        else:\n",
    "            df = df.append(pd.json_normalize(page['data']))\n",
    "        \n",
    "        if 'next_token' not in page['meta'].keys():\n",
    "            if verbose:\n",
    "                print(f'All tweets for {ACCOUNT_ID} found after '\\\n",
    "                      f'{request_count+1} API calls.')\n",
    "            break\n",
    "        \n",
    "        # Update pagination token and request_count\n",
    "        NEXT_TOKEN = page['meta']['next_token']\n",
    "        PARAMS['pagination_token'] = NEXT_TOKEN\n",
    "        request_count += 1\n",
    "        \n",
    "        if verbose: \n",
    "            print(f'Request {request_count} successful. Sleeping 1 '\\\n",
    "                  f'second and paginating.')\n",
    "        \n",
    "    if verbose:\n",
    "        print(f'All most recent for account {file_reference} downloaded.')\n",
    "        \n",
    "    if save_file:\n",
    "        # Save file with timestamp\n",
    "        fn = f\"data/{file_reference}_\"\\\n",
    "             f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.csv\"\n",
    "        print(f'Saving file to {fn}')\n",
    "        df.to_csv(fn, index=False)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def download_and_save_account_tweets(token_file_path='bearer_token.txt', \n",
    "                                   user_name='nhsuk', verbose=True, \n",
    "                                   save_file=True): \n",
    "    \"\"\"\n",
    "    This is a wrapper function for the\n",
    "    get_most_recent_tweets_account() function to download \n",
    "    the most recent tweets of a specified Twitter account\n",
    "    through the Twitter API. The request parameters\n",
    "    (i.e. variable requested) are hard-coded in this\n",
    "    function for this study. For more information \n",
    "    go to the documentation of the get_most_recent_tweets_account() \n",
    "    function via help(get_most_recent_tweets_account).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    token_file_path : str\n",
    "        Readable file path containing a document which includes\n",
    "        the Twitter API bearer token in its first line. Defaults\n",
    "        to 'bearer_token.txt'.\n",
    "    user_name : str\n",
    "        The username to look up (either including or \n",
    "        excluding) the '@' symbol.\n",
    "    verbose : bool\n",
    "        A boolean indicating whether progress of the download\n",
    "        routine should be printed to the console. Defaults to\n",
    "        True.\n",
    "    save_file : bool\n",
    "        A boolean indicating whether the resulting data frame\n",
    "        with all tweets should be saved to a csv file including\n",
    "        a timestamp in the file name (since download output\n",
    "        may depend on the time of download). Defaults to True.\n",
    "  \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        A pandas data frame including all tweets with one row\n",
    "        representing one tweet and the variables specified in\n",
    "        the PARAMS argument parsed through the\n",
    "        pandas.json_normalize() function.\n",
    "    \n",
    "    \"\"\"\n",
    "    BEARER_TOKEN = read_bearer_token(token_file_path)\n",
    "    TWITTER_USER_ID = look_up_twitter_acount_id(BEARER_TOKEN, user_name)\n",
    "    \n",
    "    # Download all variables currently available\n",
    "    PARAMS = {\n",
    "        \"max_results\": \"100\", # maximum number of results permitted\n",
    "        \"tweet.fields\": \"attachments,author_id,context_annotations,conversation_id,\"\\\n",
    "                        \"created_at,entities,geo,id,in_reply_to_user_id,lang,\"\\\n",
    "                        #\"non_public_metrics,\"\\ # not available\n",
    "                        \"public_metrics,\"\\\n",
    "                        # \"organic_metrics,\"\\  # not available\n",
    "                        # \"promoted_metrics,\"\\ # not available\n",
    "                        \"possibly_sensitive,referenced_tweets,\"\\\n",
    "                        \"reply_settings,source,text,withheld\", \n",
    "        \"user.fields\": \"created_at,description,entities,id,location,name,\"\\\n",
    "                       \"pinned_tweet_id,profile_image_url,protected,public_metrics,\"\\\n",
    "                        \"url,username,verified,withheld\",\n",
    "        \"expansions\":  \"attachments.poll_ids,attachments.media_keys,\"\\\n",
    "                       \"author_id,entities.mentions.username,geo.place_id,\"\\\n",
    "                       \"in_reply_to_user_id,referenced_tweets.id,\"\\\n",
    "                       \"referenced_tweets.id.author_id\",\n",
    "        \"media.fields\": \"duration_ms,height,media_key,preview_image_url,type,\"\\\n",
    "                        \"url,width,\"\\\n",
    "                        \"public_metrics,\"\\\n",
    "                        #\"non_public_metrics,\"\\ # not available\n",
    "                        #\"organic_metrics,promoted_metrics,\"\\ # not available\n",
    "                        \"alt_text\",\n",
    "        \"place.fields\": \"contained_within,country,country_code,full_name,\"\\\n",
    "                        \"geo,id,name,place_type\",\n",
    "        \"poll.fields\": \"duration_minutes,end_datetime,id,options,voting_status\"  \n",
    "    }\n",
    "\n",
    "    df = get_most_recent_tweets_account(TWITTER_USER_ID, BEARER_TOKEN, \n",
    "                                        PARAMS, verbose=verbose, \n",
    "                                        save_file=save_file,\n",
    "                                        file_reference=user_name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbd307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_username_from_url(text: str):\n",
    "    # remove everthing prior to and including twitter.com/\n",
    "    text = text.split('twitter.com/')[-1] \n",
    "    # get everthing prior to first / after twitter.com\n",
    "    text = text.split('/')[0]\n",
    "    # clean names\n",
    "    PERMITTED_CHARS = \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_\"\n",
    "    text = \"\".join(char for char in text if char in PERMITTED_CHARS)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ce11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a36a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter-links-for-k12-institutions-processed.csv')\n",
    "ALL_USERNAMES = pd.unique(df['link'].map(extract_username_from_url))\n",
    "downloaded_usernames = list(map(lambda f: f.split('/')[1].split('_')[0], glob.glob('data/*.csv')))\n",
    "ALL_USERNAMES = [u for u in ALL_USERNAMES if u not in downloaded_usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ba516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Downloading {len(ALL_USERNAMES)} users...')\n",
    "for user in ALL_USERNAMES:\n",
    "    print(f'\\n### Downloading {user} ...###\\n')\n",
    "    download_and_save_account_tweets(token_file_path='bearer_token.txt', \n",
    "                                     user_name=user, verbose=True, \n",
    "                                     save_file=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cea307",
   "metadata": {},
   "source": [
    "## Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c2393",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = glob.glob('data/*.csv')\n",
    "dfs = []\n",
    "for f in fs:\n",
    "    dfs.append(pd.read_csv(f))\n",
    "df = pd.concat(dfs)\n",
    "fn = f\"data/all_k12_twitter_accounts_\"\\\n",
    "     f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}.csv\"\n",
    "print(f'Saving file to {fn}')\n",
    "df.to_csv(fn, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
